
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_explain_example.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_explain_example.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_explain_example.py:


Model Explanation
=================

.. GENERATED FROM PYTHON SOURCE LINES 5-19



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_explain_example_001.png
         :alt: ROC curve for class 0, ROC curve for class 1, ROC curve for class 2
         :srcset: /auto_examples/images/sphx_glr_plot_explain_example_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_explain_example_002.png
         :alt: class: 0, class: 1, class: 2
         :srcset: /auto_examples/images/sphx_glr_plot_explain_example_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running DummyClassifier()
    accuracy: 0.391 recall_macro: 0.333 precision_macro: 0.130 f1_macro: 0.187
    === new best DummyClassifier() (using recall_macro):
    accuracy: 0.391 recall_macro: 0.333 precision_macro: 0.130 f1_macro: 0.187

    Running GaussianNB()
    accuracy: 0.970 recall_macro: 0.972 precision_macro: 0.976 f1_macro: 0.971
    === new best GaussianNB() (using recall_macro):
    accuracy: 0.970 recall_macro: 0.972 precision_macro: 0.976 f1_macro: 0.971

    Running MultinomialNB()
    accuracy: 0.932 recall_macro: 0.936 precision_macro: 0.941 f1_macro: 0.937
    Running DecisionTreeClassifier(class_weight='balanced', max_depth=1)
    accuracy: 0.572 recall_macro: 0.623 precision_macro: 0.432 f1_macro: 0.495
    Running DecisionTreeClassifier(class_weight='balanced', max_depth=5)
    accuracy: 0.933 recall_macro: 0.932 precision_macro: 0.943 f1_macro: 0.933
    Running DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease=0.01)
    accuracy: 0.940 recall_macro: 0.938 precision_macro: 0.950 f1_macro: 0.940
    Running LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000)
    accuracy: 0.978 recall_macro: 0.981 precision_macro: 0.978 f1_macro: 0.978
    === new best LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000) (using recall_macro):
    accuracy: 0.978 recall_macro: 0.981 precision_macro: 0.978 f1_macro: 0.978

    Running LogisticRegression(C=1, class_weight='balanced', max_iter=1000)
    accuracy: 0.985 recall_macro: 0.987 precision_macro: 0.986 f1_macro: 0.986
    === new best LogisticRegression(C=1, class_weight='balanced', max_iter=1000) (using recall_macro):
    accuracy: 0.985 recall_macro: 0.987 precision_macro: 0.986 f1_macro: 0.986


    Best model:
    LogisticRegression(C=1, class_weight='balanced', max_iter=1000)
    Best Scores:
    accuracy: 0.985 recall_macro: 0.987 precision_macro: 0.986 f1_macro: 0.986
                  precision    recall  f1-score   support

               0       1.00      1.00      1.00        14
               1       1.00      1.00      1.00        19
               2       1.00      1.00      1.00        12

        accuracy                           1.00        45
       macro avg       1.00      1.00      1.00        45
    weighted avg       1.00      1.00      1.00        45

    [[14  0  0]
     [ 0 19  0]
     [ 0  0 12]]
    /home/circleci/miniconda/envs/testenv/lib/python3.10/site-packages/sklearn/utils/metaestimators.py:201: FutureWarning: if_delegate_has_method was deprecated in version 1.1 and will be removed in version 1.3. Use if_available instead.
      warnings.warn(






|

.. code-block:: default

    from dabl.models import SimpleClassifier
    from dabl.explain import explain
    from sklearn.datasets import load_wine
    from sklearn.model_selection import train_test_split

    wine = load_wine()

    X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target)

    sc = SimpleClassifier()

    sc.fit(X_train, y_train)

    explain(sc, X_test, y_test)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.807 seconds)


.. _sphx_glr_download_auto_examples_plot_explain_example.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_explain_example.py <plot_explain_example.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_explain_example.ipynb <plot_explain_example.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
