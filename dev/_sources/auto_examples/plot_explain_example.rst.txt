
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_explain_example.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_explain_example.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_explain_example.py:


Model Explanation
=================

.. GENERATED FROM PYTHON SOURCE LINES 5-19



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_explain_example_001.png
         :alt: ROC curve for class 0, ROC curve for class 1, ROC curve for class 2
         :srcset: /auto_examples/images/sphx_glr_plot_explain_example_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/images/sphx_glr_plot_explain_example_002.png
         :alt: class: 0, class: 1, class: 2
         :srcset: /auto_examples/images/sphx_glr_plot_explain_example_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Running DummyClassifier()
    accuracy: 0.376 recall_macro: 0.333 precision_macro: 0.125 f1_macro: 0.182
    === new best DummyClassifier() (using recall_macro):
    accuracy: 0.376 recall_macro: 0.333 precision_macro: 0.125 f1_macro: 0.182

    Running GaussianNB()
    accuracy: 0.985 recall_macro: 0.985 precision_macro: 0.988 f1_macro: 0.986
    === new best GaussianNB() (using recall_macro):
    accuracy: 0.985 recall_macro: 0.985 precision_macro: 0.988 f1_macro: 0.986

    Running MultinomialNB()
    accuracy: 0.925 recall_macro: 0.929 precision_macro: 0.937 f1_macro: 0.930
    Running DecisionTreeClassifier(class_weight='balanced', max_depth=1)
    accuracy: 0.639 recall_macro: 0.627 precision_macro: 0.453 f1_macro: 0.519
    Running DecisionTreeClassifier(class_weight='balanced', max_depth=5)
    accuracy: 0.895 recall_macro: 0.893 precision_macro: 0.901 f1_macro: 0.892
    Running DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease=0.01)
    accuracy: 0.918 recall_macro: 0.919 precision_macro: 0.924 f1_macro: 0.916
    Running LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000)
    accuracy: 0.992 recall_macro: 0.993 precision_macro: 0.993 f1_macro: 0.993
    === new best LogisticRegression(C=0.1, class_weight='balanced', max_iter=1000) (using recall_macro):
    accuracy: 0.992 recall_macro: 0.993 precision_macro: 0.993 f1_macro: 0.993

    Running LogisticRegression(C=1, class_weight='balanced', max_iter=1000)
    accuracy: 1.000 recall_macro: 1.000 precision_macro: 1.000 f1_macro: 1.000
    === new best LogisticRegression(C=1, class_weight='balanced', max_iter=1000) (using recall_macro):
    accuracy: 1.000 recall_macro: 1.000 precision_macro: 1.000 f1_macro: 1.000


    Best model:
    LogisticRegression(C=1, class_weight='balanced', max_iter=1000)
    Best Scores:
    accuracy: 1.000 recall_macro: 1.000 precision_macro: 1.000 f1_macro: 1.000
                  precision    recall  f1-score   support

               0       0.87      1.00      0.93        13
               1       1.00      0.86      0.92        21
               2       0.92      1.00      0.96        11

        accuracy                           0.93        45
       macro avg       0.93      0.95      0.94        45
    weighted avg       0.94      0.93      0.93        45

    [[13  0  0]
     [ 2 18  1]
     [ 0  0 11]]
    /home/circleci/miniconda/envs/testenv/lib/python3.10/site-packages/sklearn/utils/metaestimators.py:201: FutureWarning: if_delegate_has_method was deprecated in version 1.1 and will be removed in version 1.3. Use if_available instead.
      warnings.warn(






|

.. code-block:: default

    from dabl.models import SimpleClassifier
    from dabl.explain import explain
    from sklearn.datasets import load_wine
    from sklearn.model_selection import train_test_split

    wine = load_wine()

    X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target)

    sc = SimpleClassifier()

    sc.fit(X_train, y_train)

    explain(sc, X_test, y_test)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.721 seconds)


.. _sphx_glr_download_auto_examples_plot_explain_example.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_explain_example.py <plot_explain_example.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_explain_example.ipynb <plot_explain_example.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
